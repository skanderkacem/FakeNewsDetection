{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç D√©tection de Fake News Multimodale (Texte + Image)\n",
    "\n",
    "Ce notebook impl√©mente un syst√®me complet de d√©tection de fake news utilisant √† la fois le contenu textuel et les images des articles.\n",
    "\n",
    "## Structure du projet :\n",
    "1. **Chargement des donn√©es** - Fusion automatique des fichiers CSV\n",
    "2. **Pr√©traitement** - Nettoyage et t√©l√©chargement des images\n",
    "3. **Analyse exploratoire** - Statistiques et visualisations\n",
    "4. **Pr√©paration des donn√©es** - Encodage BERT + CNN features\n",
    "5. **Mod√©lisation multimodale** - Fusion des embeddings\n",
    "6. **√âvaluation** - M√©triques de performance\n",
    "7. **Test en temps r√©el** - Interface de pr√©diction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Installation des d√©pendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: torchvision in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (0.19.1)\n",
      "Requirement already satisfied: transformers in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (4.46.3)\n",
      "Requirement already satisfied: pandas in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: numpy in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (1.24.4)\n",
      "Requirement already satisfied: matplotlib in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (3.7.5)\n",
      "Requirement already satisfied: seaborn in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: sympy in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: filelock in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: fsspec in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: networkx in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from transformers) (0.33.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from matplotlib) (6.4.5)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.20.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from requests->transformers) (2025.6.15)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.3.2-cp38-cp38-win_amd64.whl (9.3 MB)\n",
      "Requirement already satisfied: requests in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (2.32.4)\n",
      "Requirement already satisfied: pillow in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (10.4.0)\n",
      "Requirement already satisfied: tqdm in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (4.67.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement hashlib2 (from versions: none)\n",
      "ERROR: No matching distribution found for hashlib2\n",
      "WARNING: You are using pip version 21.1.1; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: accelerate in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from datasets) (3.10.11)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from datasets) (1.24.4)\n",
      "Requirement already satisfied: packaging in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from datasets) (2024.9.0)\n",
      "Requirement already satisfied: pandas in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from datasets) (0.33.2)\n",
      "Requirement already satisfied: xxhash in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: filelock in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from aiohttp->datasets) (1.15.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from huggingface-hub>=0.23.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: colorama in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: psutil in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from accelerate) (2.4.1)\n",
      "Requirement already satisfied: jinja2 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: sympy in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\work( sous disque c)\\4eme esprit\\stage d'√©t√©\\fakenews project\\env\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# Installation des packages n√©cessaires\n",
    "!pip install torch torchvision transformers pandas numpy matplotlib seaborn\n",
    "!pip install scikit-learn requests pillow tqdm hashlib2\n",
    "!pip install datasets accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Imports et configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModel\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Sklearn pour les m√©triques\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, f1_score, classification_report, confusion_matrix\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m resample\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import hashlib\n",
    "import requests\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "# Transformers pour BERT\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Sklearn pour les m√©triques\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device utilis√©: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1Ô∏è‚É£ Chargement des donn√©es\n",
    "\n",
    "Chargement automatique de tous les fichiers CSV du dossier FakeNewsNetData avec d√©tection automatique des labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_csv_files(data_folder='FakeNewsNetData'):\n",
    "    \"\"\"\n",
    "    Charge tous les fichiers CSV du dossier et assigne automatiquement les labels\n",
    "    selon le nom du fichier (fake = 0, real = 1)\n",
    "    \"\"\"\n",
    "    all_dataframes = []\n",
    "    csv_files = glob.glob(os.path.join(data_folder, '*.csv'))\n",
    "    \n",
    "    print(f\"üìÅ Fichiers CSV trouv√©s: {len(csv_files)}\")\n",
    "    \n",
    "    for file_path in csv_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        print(f\"\\nüìÑ Traitement de: {filename}\")\n",
    "        \n",
    "        try:\n",
    "            # Lecture du fichier CSV\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # D√©tection automatique du label selon le nom du fichier\n",
    "            if 'fake' in filename.lower():\n",
    "                df['label'] = 0  # Fake news\n",
    "                label_type = \"FAKE\"\n",
    "            elif 'real' in filename.lower():\n",
    "                df['label'] = 1  # Real news\n",
    "                label_type = \"REAL\"\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Impossible de d√©terminer le label pour {filename}\")\n",
    "                continue\n",
    "            \n",
    "            # Ajout de la source du fichier\n",
    "            df['source_file'] = filename\n",
    "            \n",
    "            print(f\"   ‚úÖ {len(df)} articles charg√©s - Label: {label_type}\")\n",
    "            print(f\"   üìä Colonnes: {list(df.columns)}\")\n",
    "            \n",
    "            all_dataframes.append(df)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Erreur lors du chargement de {filename}: {str(e)}\")\n",
    "    \n",
    "    # Fusion de tous les DataFrames\n",
    "    if all_dataframes:\n",
    "        df_all = pd.concat(all_dataframes, ignore_index=True, sort=False)\n",
    "        print(f\"\\nüéØ TOTAL: {len(df_all)} articles charg√©s\")\n",
    "        print(f\"üìà R√©partition des labels:\")\n",
    "        print(df_all['label'].value_counts())\n",
    "        return df_all\n",
    "    else:\n",
    "        print(\"‚ùå Aucun fichier n'a pu √™tre charg√©\")\n",
    "        return None\n",
    "\n",
    "# Chargement des donn√©es\n",
    "df_all = load_all_csv_files()\n",
    "print(f\"\\nüìã Colonnes disponibles: {list(df_all.columns)}\")\n",
    "print(f\"üìè Shape du dataset: {df_all.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2Ô∏è‚É£ Pr√©traitement des donn√©es\n",
    "\n",
    "Nettoyage des donn√©es, gestion des URLs d'images et t√©l√©chargement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Pr√©traitement complet des donn√©es:\n",
    "    - Suppression des lignes avec title/text manquants\n",
    "    - Cr√©ation d'une colonne image_url unifi√©e\n",
    "    - Nettoyage des donn√©es\n",
    "    \"\"\"\n",
    "    print(\"üßπ D√©but du pr√©traitement...\")\n",
    "    initial_count = len(df)\n",
    "    \n",
    "    # 1. Suppression des lignes avec title ou text manquants\n",
    "    print(f\"\\n1Ô∏è‚É£ Suppression des lignes avec title/text manquants...\")\n",
    "    df = df.dropna(subset=['title'])\n",
    "    print(f\"   Apr√®s suppression title manquant: {len(df)} articles\")\n",
    "    \n",
    "    # Gestion de la colonne text (peut √™tre 'text' ou autre selon le fichier)\n",
    "    text_columns = [col for col in df.columns if 'text' in col.lower()]\n",
    "    if text_columns:\n",
    "        main_text_col = text_columns[0]\n",
    "        df = df.dropna(subset=[main_text_col])\n",
    "        # Renommer la colonne en 'text' pour uniformiser\n",
    "        if main_text_col != 'text':\n",
    "            df = df.rename(columns={main_text_col: 'text'})\n",
    "        print(f\"   Apr√®s suppression text manquant: {len(df)} articles\")\n",
    "    \n",
    "    # 2. Cr√©ation d'une colonne image_url unifi√©e\n",
    "    print(f\"\\n2Ô∏è‚É£ Cr√©ation de la colonne image_url unifi√©e...\")\n",
    "    \n",
    "    # Initialiser la colonne image_url\n",
    "    df['image_url'] = None\n",
    "    \n",
    "    # Priorit√©: image_url > top_img > images\n",
    "    if 'image_url' in df.columns and df['image_url'].notna().any():\n",
    "        df['image_url'] = df['image_url'].fillna('')\n",
    "    \n",
    "    if 'top_img' in df.columns:\n",
    "        df['image_url'] = df['image_url'].fillna(df['top_img'])\n",
    "    \n",
    "    if 'images' in df.columns:\n",
    "        # Extraire la premi√®re image de la liste si c'est une cha√Æne\n",
    "        def extract_first_image(images_str):\n",
    "            if pd.isna(images_str) or images_str == '':\n",
    "                return None\n",
    "            # Si c'est une liste d'URLs s√©par√©es par des espaces ou virgules\n",
    "            images = str(images_str).split()\n",
    "            return images[0] if images else None\n",
    "        \n",
    "        df['temp_images'] = df['images'].apply(extract_first_image)\n",
    "        df['image_url'] = df['image_url'].fillna(df['temp_images'])\n",
    "        df = df.drop('temp_images', axis=1)\n",
    "    \n",
    "    # Nettoyage des URLs d'images\n",
    "    df['image_url'] = df['image_url'].astype(str)\n",
    "    df['image_url'] = df['image_url'].replace(['nan', 'None', ''], None)\n",
    "    \n",
    "    # Filtrer les URLs valides\n",
    "    def is_valid_image_url(url):\n",
    "        if pd.isna(url) or url is None:\n",
    "            return False\n",
    "        url = str(url).strip()\n",
    "        return url.startswith(('http://', 'https://')) and len(url) > 10\n",
    "    \n",
    "    df['has_valid_image_url'] = df['image_url'].apply(is_valid_image_url)\n",
    "    \n",
    "    print(f\"   Articles avec URL d'image valide: {df['has_valid_image_url'].sum()}\")\n",
    "    print(f\"   Articles sans URL d'image: {(~df['has_valid_image_url']).sum()}\")\n",
    "    \n",
    "    # 3. Nettoyage des textes\n",
    "    print(f\"\\n3Ô∏è‚É£ Nettoyage des textes...\")\n",
    "    \n",
    "    def clean_text(text):\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        text = str(text).strip()\n",
    "        # Supprimer les caract√®res de contr√¥le\n",
    "        text = ''.join(char for char in text if ord(char) >= 32 or char in '\\n\\t')\n",
    "        return text\n",
    "    \n",
    "    df['title'] = df['title'].apply(clean_text)\n",
    "    df['text'] = df['text'].apply(clean_text)\n",
    "    \n",
    "    # Supprimer les articles avec des textes trop courts\n",
    "    df = df[df['title'].str.len() >= 10]\n",
    "    df = df[df['text'].str.len() >= 50]\n",
    "    \n",
    "    print(f\"   Apr√®s nettoyage des textes: {len(df)} articles\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Pr√©traitement termin√©: {initial_count} ‚Üí {len(df)} articles ({len(df)/initial_count*100:.1f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Pr√©traitement des donn√©es\n",
    "df_clean = preprocess_data(df_all)\n",
    "print(f\"\\nüìä R√©partition finale des labels:\")\n",
    "print(df_clean['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• T√©l√©chargement des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_images(df, images_folder='images_final', max_images=None):\n",
    "    \"\"\"\n",
    "    T√©l√©charge toutes les images valides et met √† jour le DataFrame\n",
    "    avec les chemins locaux des images t√©l√©charg√©es\n",
    "    \"\"\"\n",
    "    print(f\"üì• D√©but du t√©l√©chargement des images dans {images_folder}/\")\n",
    "    \n",
    "    # Cr√©er le dossier d'images\n",
    "    os.makedirs(images_folder, exist_ok=True)\n",
    "    \n",
    "    # Filtrer les articles avec des URLs d'images valides\n",
    "    df_with_images = df[df['has_valid_image_url']].copy()\n",
    "    \n",
    "    if max_images:\n",
    "        df_with_images = df_with_images.head(max_images)\n",
    "    \n",
    "    print(f\"üéØ {len(df_with_images)} images √† t√©l√©charger\")\n",
    "    \n",
    "    successful_downloads = []\n",
    "    failed_downloads = []\n",
    "    \n",
    "    for idx, row in tqdm(df_with_images.iterrows(), total=len(df_with_images), desc=\"T√©l√©chargement\"):\n",
    "        try:\n",
    "            image_url = row['image_url']\n",
    "            \n",
    "            # Cr√©er un nom de fichier unique avec hash MD5\n",
    "            url_hash = hashlib.md5(image_url.encode()).hexdigest()\n",
    "            \n",
    "            # D√©terminer l'extension du fichier\n",
    "            if image_url.lower().endswith(('.jpg', '.jpeg')):\n",
    "                ext = '.jpg'\n",
    "            elif image_url.lower().endswith('.png'):\n",
    "                ext = '.png'\n",
    "            elif image_url.lower().endswith('.gif'):\n",
    "                ext = '.gif'\n",
    "            else:\n",
    "                ext = '.jpg'  # Par d√©faut\n",
    "            \n",
    "            filename = f\"{url_hash}{ext}\"\n",
    "            filepath = os.path.join(images_folder, filename)\n",
    "            \n",
    "            # V√©rifier si l'image existe d√©j√†\n",
    "            if os.path.exists(filepath):\n",
    "                successful_downloads.append((idx, filepath))\n",
    "                continue\n",
    "            \n",
    "            # T√©l√©charger l'image\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "            }\n",
    "            \n",
    "            response = requests.get(image_url, headers=headers, timeout=10, stream=True)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # V√©rifier que c'est bien une image\n",
    "            content_type = response.headers.get('content-type', '')\n",
    "            if not content_type.startswith('image/'):\n",
    "                failed_downloads.append((idx, f\"Type de contenu invalide: {content_type}\"))\n",
    "                continue\n",
    "            \n",
    "            # Sauvegarder l'image\n",
    "            with open(filepath, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            \n",
    "            # V√©rifier que l'image peut √™tre ouverte\n",
    "            try:\n",
    "                with Image.open(filepath) as img:\n",
    "                    img.verify()\n",
    "                successful_downloads.append((idx, filepath))\n",
    "            except Exception:\n",
    "                os.remove(filepath)\n",
    "                failed_downloads.append((idx, \"Image corrompue\"))\n",
    "            \n",
    "        except Exception as e:\n",
    "            failed_downloads.append((idx, str(e)))\n",
    "    \n",
    "    print(f\"\\n‚úÖ T√©l√©chargements r√©ussis: {len(successful_downloads)}\")\n",
    "    print(f\"‚ùå T√©l√©chargements √©chou√©s: {len(failed_downloads)}\")\n",
    "    \n",
    "    # Mettre √† jour le DataFrame avec les chemins des images\n",
    "    df['image_path'] = None\n",
    "    for idx, filepath in successful_downloads:\n",
    "        df.loc[idx, 'image_path'] = filepath\n",
    "    \n",
    "    # Filtrer pour ne garder que les articles avec images t√©l√©charg√©es\n",
    "    df_final = df[df['image_path'].notna()].copy()\n",
    "    \n",
    "    print(f\"\\nüéØ Dataset final: {len(df_final)} articles avec images\")\n",
    "    print(f\"üìä R√©partition des labels:\")\n",
    "    print(df_final['label'].value_counts())\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "# T√©l√©chargement des images (limit√© √† 1000 pour les tests)\n",
    "df_with_images = download_images(df_clean, max_images=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ √âquilibrage des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataset(df, min_samples=200, method='undersample'):\n",
    "    \"\"\"\n",
    "    √âquilibre le dataset selon la m√©thode choisie\n",
    "    \"\"\"\n",
    "    print(f\"‚öñÔ∏è √âquilibrage du dataset (m√©thode: {method})\")\n",
    "    \n",
    "    label_counts = df['label'].value_counts()\n",
    "    print(f\"\\nüìä R√©partition actuelle:\")\n",
    "    print(label_counts)\n",
    "    \n",
    "    # V√©rifier si nous avons assez de donn√©es\n",
    "    if label_counts.min() < min_samples:\n",
    "        print(f\"\\n‚ö†Ô∏è Pas assez de donn√©es (minimum: {min_samples} par classe)\")\n",
    "        print(f\"üîÑ Application de l'oversampling...\")\n",
    "        \n",
    "        # Oversampling de la classe minoritaire\n",
    "        df_majority = df[df['label'] == label_counts.idxmax()]\n",
    "        df_minority = df[df['label'] == label_counts.idxmin()]\n",
    "        \n",
    "        # Calculer le nombre d'√©chantillons √† g√©n√©rer\n",
    "        target_size = max(min_samples, len(df_majority))\n",
    "        \n",
    "        # Oversampling avec remplacement\n",
    "        df_minority_upsampled = resample(df_minority, \n",
    "                                       replace=True,\n",
    "                                       n_samples=target_size,\n",
    "                                       random_state=42)\n",
    "        \n",
    "        df_balanced = pd.concat([df_majority, df_minority_upsampled])\n",
    "        \n",
    "    elif method == 'undersample':\n",
    "        # Undersampling - prendre le minimum\n",
    "        min_count = label_counts.min()\n",
    "        \n",
    "        df_fake = df[df['label'] == 0].sample(n=min_count, random_state=42)\n",
    "        df_real = df[df['label'] == 1].sample(n=min_count, random_state=42)\n",
    "        \n",
    "        df_balanced = pd.concat([df_fake, df_real])\n",
    "        \n",
    "    elif method == 'oversample':\n",
    "        # Oversampling - √©galiser au maximum\n",
    "        max_count = label_counts.max()\n",
    "        \n",
    "        df_majority = df[df['label'] == label_counts.idxmax()]\n",
    "        df_minority = df[df['label'] == label_counts.idxmin()]\n",
    "        \n",
    "        df_minority_upsampled = resample(df_minority,\n",
    "                                       replace=True,\n",
    "                                       n_samples=max_count,\n",
    "                                       random_state=42)\n",
    "        \n",
    "        df_balanced = pd.concat([df_majority, df_minority_upsampled])\n",
    "    \n",
    "    # M√©langer les donn√©es\n",
    "    df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Dataset √©quilibr√©:\")\n",
    "    print(df_balanced['label'].value_counts())\n",
    "    print(f\"üìè Taille finale: {len(df_balanced)} articles\")\n",
    "    \n",
    "    return df_balanced\n",
    "\n",
    "# √âquilibrage du dataset\n",
    "df_balanced = balance_dataset(df_with_images, min_samples=200, method='oversample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3Ô∏è‚É£ Analyse exploratoire des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploratory_analysis(df):\n",
    "    \"\"\"\n",
    "    Analyse exploratoire compl√®te du dataset\n",
    "    \"\"\"\n",
    "    print(\"üìä ANALYSE EXPLORATOIRE DES DONN√âES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Statistiques g√©n√©rales\n",
    "    print(f\"\\n1Ô∏è‚É£ STATISTIQUES G√âN√âRALES\")\n",
    "    print(f\"   üìè Nombre total d'articles: {len(df)}\")\n",
    "    print(f\"   üìä R√©partition des labels:\")\n",
    "    label_counts = df['label'].value_counts()\n",
    "    for label, count in label_counts.items():\n",
    "        label_name = \"FAKE\" if label == 0 else \"REAL\"\n",
    "        percentage = count / len(df) * 100\n",
    "        print(f\"      {label_name}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # 2. Analyse des textes\n",
    "    print(f\"\\n2Ô∏è‚É£ ANALYSE DES TEXTES\")\n",
    "    \n",
    "    # Longueurs des titres\n",
    "    df['title_length'] = df['title'].str.len()\n",
    "    df['text_length'] = df['text'].str.len()\n",
    "    df['title_words'] = df['title'].str.split().str.len()\n",
    "    df['text_words'] = df['text'].str.split().str.len()\n",
    "    \n",
    "    print(f\"   üìù Longueur des titres (caract√®res):\")\n",
    "    print(f\"      Moyenne: {df['title_length'].mean():.1f}\")\n",
    "    print(f\"      M√©diane: {df['title_length'].median():.1f}\")\n",
    "    print(f\"      Min-Max: {df['title_length'].min()}-{df['title_length'].max()}\")\n",
    "    \n",
    "    print(f\"   üìÑ Longueur des textes (caract√®res):\")\n",
    "    print(f\"      Moyenne: {df['text_length'].mean():.1f}\")\n",
    "    print(f\"      M√©diane: {df['text_length'].median():.1f}\")\n",
    "    print(f\"      Min-Max: {df['text_length'].min()}-{df['text_length'].max()}\")\n",
    "    \n",
    "    # 3. Exemples d'articles\n",
    "    print(f\"\\n3Ô∏è‚É£ EXEMPLES D'ARTICLES\")\n",
    "    \n",
    "    print(f\"\\nüî¥ EXEMPLE FAKE NEWS:\")\n",
    "    fake_example = df[df['label'] == 0].iloc[0]\n",
    "    print(f\"   Titre: {fake_example['title'][:100]}...\")\n",
    "    print(f\"   Texte: {fake_example['text'][:200]}...\")\n",
    "    print(f\"   Image: {fake_example['image_path']}\")\n",
    "    \n",
    "    print(f\"\\nüü¢ EXEMPLE REAL NEWS:\")\n",
    "    real_example = df[df['label'] == 1].iloc[0]\n",
    "    print(f\"   Titre: {real_example['title'][:100]}...\")\n",
    "    print(f\"   Texte: {real_example['text'][:200]}...\")\n",
    "    print(f\"   Image: {real_example['image_path']}\")\n",
    "    \n",
    "    # 4. Visualisations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Distribution des labels\n",
    "    label_counts.plot(kind='bar', ax=axes[0,0], color=['red', 'green'])\n",
    "    axes[0,0].set_title('R√©partition des Labels')\n",
    "    axes[0,0].set_xlabel('Label (0=Fake, 1=Real)')\n",
    "    axes[0,0].set_ylabel('Nombre d\\'articles')\n",
    "    \n",
    "    # Distribution des longueurs de titres par label\n",
    "    df.boxplot(column='title_length', by='label', ax=axes[0,1])\n",
    "    axes[0,1].set_title('Longueur des Titres par Label')\n",
    "    axes[0,1].set_xlabel('Label (0=Fake, 1=Real)')\n",
    "    axes[0,1].set_ylabel('Longueur (caract√®res)')\n",
    "    \n",
    "    # Distribution des longueurs de textes par label\n",
    "    df.boxplot(column='text_length', by='label', ax=axes[1,0])\n",
    "    axes[1,0].set_title('Longueur des Textes par Label')\n",
    "    axes[1,0].set_xlabel('Label (0=Fake, 1=Real)')\n",
    "    axes[1,0].set_ylabel('Longueur (caract√®res)')\n",
    "    \n",
    "    # Histogramme des longueurs de titres\n",
    "    df[df['label']==0]['title_length'].hist(alpha=0.5, label='Fake', bins=30, ax=axes[1,1], color='red')\n",
    "    df[df['label']==1]['title_length'].hist(alpha=0.5, label='Real', bins=30, ax=axes[1,1], color='green')\n",
    "    axes[1,1].set_title('Distribution des Longueurs de Titres')\n",
    "    axes[1,1].set_xlabel('Longueur (caract√®res)')\n",
    "    axes[1,1].set_ylabel('Fr√©quence')\n",
    "    axes[1,1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Analyse exploratoire\n",
    "df_analyzed = exploratory_analysis(df_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4Ô∏è‚É£ Pr√©paration des donn√©es pour la mod√©lisation\n",
    "\n",
    "Encodage des textes avec BERT et extraction des features d'images avec CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration des mod√®les\n",
    "BERT_MODEL = 'distilbert-base-uncased'\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 16\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "# Initialisation du tokenizer BERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL)\n",
    "bert_model = AutoModel.from_pretrained(BERT_MODEL)\n",
    "\n",
    "print(f\"‚úÖ Mod√®le BERT charg√©: {BERT_MODEL}\")\n",
    "print(f\"üñºÔ∏è Taille des images: {IMAGE_SIZE}x{IMAGE_SIZE}\")\n",
    "print(f\"üì¶ Taille des batches: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset personnalis√© pour les donn√©es multimodales (texte + image)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df, tokenizer, max_length=512, image_size=224, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        # Transformations pour les images\n",
    "        if transform is None:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((image_size, image_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                   std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # Pr√©paration du texte (titre + texte)\n",
    "        text = f\"{row['title']} [SEP] {row['text']}\"\n",
    "        \n",
    "        # Tokenisation\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Chargement et transformation de l'image\n",
    "        try:\n",
    "            image = Image.open(row['image_path']).convert('RGB')\n",
    "            image = self.transform(image)\n",
    "        except Exception as e:\n",
    "            # Image par d√©faut en cas d'erreur\n",
    "            image = torch.zeros(3, self.image_size, self.image_size)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'image': image,\n",
    "            'label': torch.tensor(row['label'], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ Classe MultimodalDataset d√©finie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalFakeNewsDetector(nn.Module):\n",
    "    \"\"\"\n",
    "    Mod√®le multimodal combinant BERT pour le texte et ResNet pour les images\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, bert_model_name='distilbert-base-uncased', num_classes=2, dropout=0.3):\n",
    "        super(MultimodalFakeNewsDetector, self).__init__()\n",
    "        \n",
    "        # Encodeur de texte (BERT)\n",
    "        self.bert = AutoModel.from_pretrained(bert_model_name)\n",
    "        self.bert_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Encodeur d'image (ResNet18 pr√©-entra√Æn√©)\n",
    "        self.resnet = resnet18(pretrained=True)\n",
    "        # Supprimer la derni√®re couche de classification\n",
    "        self.resnet = nn.Sequential(*list(self.resnet.children())[:-1])\n",
    "        self.image_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Dimensions des features\n",
    "        self.bert_dim = 768  # DistilBERT hidden size\n",
    "        self.image_dim = 512  # ResNet18 feature size\n",
    "        \n",
    "        # Couches de fusion\n",
    "        self.fusion_dim = 256\n",
    "        self.text_projection = nn.Linear(self.bert_dim, self.fusion_dim)\n",
    "        self.image_projection = nn.Linear(self.image_dim, self.fusion_dim)\n",
    "        \n",
    "        # Couches de classification\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.fusion_dim * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, image):\n",
    "        # Encodage du texte avec BERT\n",
    "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_features = bert_output.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "        text_features = self.bert_dropout(text_features)\n",
    "        \n",
    "        # Encodage de l'image avec ResNet\n",
    "        image_features = self.resnet(image)\n",
    "        image_features = image_features.view(image_features.size(0), -1)  # Flatten\n",
    "        image_features = self.image_dropout(image_features)\n",
    "        \n",
    "        # Projection vers l'espace de fusion\n",
    "        text_projected = self.text_projection(text_features)\n",
    "        image_projected = self.image_projection(image_features)\n",
    "        \n",
    "        # Fusion des modalit√©s (concat√©nation)\n",
    "        fused_features = torch.cat([text_projected, image_projected], dim=1)\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(fused_features)\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\"‚úÖ Mod√®le MultimodalFakeNewsDetector d√©fini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S√©paration des donn√©es en train/test\n",
    "train_df, test_df = train_test_split(\n",
    "    df_analyzed, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=df_analyzed['label']\n",
    ")\n",
    "\n",
    "print(f\"üìä Donn√©es d'entra√Ænement: {len(train_df)} articles\")\n",
    "print(f\"üìä Donn√©es de test: {len(test_df)} articles\")\n",
    "print(f\"\\nüéØ R√©partition train:\")\n",
    "print(train_df['label'].value_counts())\n",
    "print(f\"\\nüéØ R√©partition test:\")\n",
    "print(test_df['label'].value_counts())\n",
    "\n",
    "# Cr√©ation des datasets\n",
    "train_dataset = MultimodalDataset(train_df, tokenizer, MAX_LENGTH, IMAGE_SIZE)\n",
    "test_dataset = MultimodalDataset(test_df, tokenizer, MAX_LENGTH, IMAGE_SIZE)\n",
    "\n",
    "# Cr√©ation des dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Dataloaders cr√©√©s\")\n",
    "print(f\"   üì¶ Batches d'entra√Ænement: {len(train_loader)}\")\n",
    "print(f\"   üì¶ Batches de test: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5Ô∏è‚É£ Entra√Ænement du mod√®le multimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, num_epochs=5, learning_rate=2e-5):\n",
    "    \"\"\"\n",
    "    Entra√Æne le mod√®le multimodal\n",
    "    \"\"\"\n",
    "    print(f\"üöÄ D√©but de l'entra√Ænement sur {device}\")\n",
    "    print(f\"üìä Param√®tres: {num_epochs} epochs, LR={learning_rate}\")\n",
    "    \n",
    "    # D√©placer le mod√®le sur le device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Optimiseur et fonction de perte\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Scheduler pour ajuster le learning rate\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "    \n",
    "    # Historique des m√©triques\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nüìÖ Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Phase d'entra√Ænement\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=\"Entra√Ænement\")\n",
    "        for batch in train_pbar:\n",
    "            # D√©placer les donn√©es sur le device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask, images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistiques\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Mise √† jour de la barre de progression\n",
    "            train_acc = 100 * train_correct / train_total\n",
    "            train_pbar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'Acc': f'{train_acc:.2f}%'\n",
    "            })\n",
    "        \n",
    "        # Moyennes pour l'epoch\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_train_acc = 100 * train_correct / train_total\n",
    "        \n",
    "        # Phase de validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(test_loader, desc=\"Validation\")\n",
    "            for batch in val_pbar:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                images = batch['image'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids, attention_mask, images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                val_acc = 100 * val_correct / val_total\n",
    "                val_pbar.set_postfix({\n",
    "                    'Loss': f'{loss.item():.4f}',\n",
    "                    'Acc': f'{val_acc:.2f}%'\n",
    "                })\n",
    "        \n",
    "        avg_val_loss = val_loss / len(test_loader)\n",
    "        avg_val_acc = 100 * val_correct / val_total\n",
    "        \n",
    "        # Sauvegarde du meilleur mod√®le\n",
    "        if avg_val_acc > best_val_acc:\n",
    "            best_val_acc = avg_val_acc\n",
    "            torch.save(model.state_dict(), 'best_multimodal_model.pth')\n",
    "            print(f\"üíæ Nouveau meilleur mod√®le sauvegard√© (Acc: {best_val_acc:.2f}%)\")\n",
    "        \n",
    "        # Mise √† jour du scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Stockage des m√©triques\n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_accuracies.append(avg_train_acc)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_accuracies.append(avg_val_acc)\n",
    "        \n",
    "        # Affichage des r√©sultats de l'epoch\n",
    "        print(f\"\\nüìä R√©sultats Epoch {epoch+1}:\")\n",
    "        print(f\"   üèãÔ∏è Train - Loss: {avg_train_loss:.4f}, Acc: {avg_train_acc:.2f}%\")\n",
    "        print(f\"   üéØ Val   - Loss: {avg_val_loss:.4f}, Acc: {avg_val_acc:.2f}%\")\n",
    "        print(f\"   üìà LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "    \n",
    "    print(f\"\\nüéâ Entra√Ænement termin√©!\")\n",
    "    print(f\"üèÜ Meilleure accuracy de validation: {best_val_acc:.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'val_losses': val_losses,\n",
    "        'val_accuracies': val_accuracies,\n",
    "        'best_val_acc': best_val_acc\n",
    "    }\n",
    "\n",
    "# Initialisation et entra√Ænement du mod√®le\n",
    "model = MultimodalFakeNewsDetector(BERT_MODEL, num_classes=2, dropout=0.3)\n",
    "print(f\"\\nüß† Mod√®le initialis√©\")\n",
    "print(f\"üìä Nombre de param√®tres: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Entra√Ænement\n",
    "history = train_model(model, train_loader, test_loader, num_epochs=3, learning_rate=2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Visualisation des courbes d'entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Affiche les courbes de loss et d'accuracy\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    epochs = range(1, len(history['train_losses']) + 1)\n",
    "    \n",
    "    # Courbes de loss\n",
    "    ax1.plot(epochs, history['train_losses'], 'b-', label='Train Loss', marker='o')\n",
    "    ax1.plot(epochs, history['val_losses'], 'r-', label='Validation Loss', marker='s')\n",
    "    ax1.set_title('√âvolution de la Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Courbes d'accuracy\n",
    "    ax2.plot(epochs, history['train_accuracies'], 'b-', label='Train Accuracy', marker='o')\n",
    "    ax2.plot(epochs, history['val_accuracies'], 'r-', label='Validation Accuracy', marker='s')\n",
    "    ax2.set_title('√âvolution de l\\'Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Affichage des m√©triques finales\n",
    "    print(f\"üìä R√âSULTATS FINAUX:\")\n",
    "    print(f\"   üèãÔ∏è Train Accuracy: {history['train_accuracies'][-1]:.2f}%\")\n",
    "    print(f\"   üéØ Validation Accuracy: {history['val_accuracies'][-1]:.2f}%\")\n",
    "    print(f\"   üèÜ Meilleure Val Accuracy: {history['best_val_acc']:.2f}%\")\n",
    "\n",
    "# Affichage des courbes\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6Ô∏è‚É£ √âvaluation d√©taill√©e du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, class_names=['Fake', 'Real']):\n",
    "    \"\"\"\n",
    "    √âvaluation compl√®te du mod√®le avec m√©triques d√©taill√©es\n",
    "    \"\"\"\n",
    "    print(\"üîç √âVALUATION D√âTAILL√âE DU MOD√àLE\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Charger le meilleur mod√®le\n",
    "    model.load_state_dict(torch.load('best_multimodal_model.pth'))\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_probabilities = []\n",
    "    \n",
    "    print(\"üìä Pr√©diction sur le jeu de test...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"√âvaluation\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask, images)\n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probabilities.extend(probabilities.cpu().numpy())\n",
    "    \n",
    "    # Conversion en arrays numpy\n",
    "    y_true = np.array(all_labels)\n",
    "    y_pred = np.array(all_predictions)\n",
    "    y_prob = np.array(all_probabilities)\n",
    "    \n",
    "    # Calcul des m√©triques\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"\\nüìà M√âTRIQUES GLOBALES:\")\n",
    "    print(f\"   üéØ Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"   üìä F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    # Rapport de classification d√©taill√©\n",
    "    print(f\"\\nüìã RAPPORT DE CLASSIFICATION:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "    \n",
    "    # Matrice de confusion\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Matrice de Confusion')\n",
    "    plt.xlabel('Pr√©dictions')\n",
    "    plt.ylabel('Vraies √©tiquettes')\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyse des erreurs\n",
    "    print(f\"\\nüîç ANALYSE DES ERREURS:\")\n",
    "    \n",
    "    # Faux positifs (pr√©dits comme Real mais sont Fake)\n",
    "    false_positives = (y_true == 0) & (y_pred == 1)\n",
    "    fp_count = np.sum(false_positives)\n",
    "    \n",
    "    # Faux n√©gatifs (pr√©dits comme Fake mais sont Real)\n",
    "    false_negatives = (y_true == 1) & (y_pred == 0)\n",
    "    fn_count = np.sum(false_negatives)\n",
    "    \n",
    "    print(f\"   üî¥ Faux Positifs (Fake ‚Üí Real): {fp_count}\")\n",
    "    print(f\"   üü° Faux N√©gatifs (Real ‚Üí Fake): {fn_count}\")\n",
    "    \n",
    "    # Distribution des probabilit√©s\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(y_prob[y_true == 0, 0], alpha=0.5, label='Fake (vraies)', bins=20, color='red')\n",
    "    plt.hist(y_prob[y_true == 1, 0], alpha=0.5, label='Real (vraies)', bins=20, color='green')\n",
    "    plt.xlabel('Probabilit√© de Fake')\n",
    "    plt.ylabel('Fr√©quence')\n",
    "    plt.title('Distribution des Probabilit√©s - Classe Fake')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(y_prob[y_true == 0, 1], alpha=0.5, label='Fake (vraies)', bins=20, color='red')\n",
    "    plt.hist(y_prob[y_true == 1, 1], alpha=0.5, label='Real (vraies)', bins=20, color='green')\n",
    "    plt.xlabel('Probabilit√© de Real')\n",
    "    plt.ylabel('Fr√©quence')\n",
    "    plt.title('Distribution des Probabilit√©s - Classe Real')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_score': f1,\n",
    "        'y_true': y_true,\n",
    "        'y_pred': y_pred,\n",
    "        'y_prob': y_prob,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "# √âvaluation du mod√®le\n",
    "evaluation_results = evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7Ô∏è‚É£ Test en temps r√©el - Interface de pr√©diction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fake_news(model, tokenizer, title, text, image_path, device=device):\n",
    "    \"\"\"\n",
    "    Pr√©dit si un article est fake ou real √† partir du titre, texte et image\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Pr√©paration du texte\n",
    "    combined_text = f\"{title} [SEP] {text}\"\n",
    "    \n",
    "    # Tokenisation\n",
    "    encoding = tokenizer(\n",
    "        combined_text,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Pr√©paration de l'image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = transform(image).unsqueeze(0)  # Ajouter dimension batch\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur lors du chargement de l'image: {e}\")\n",
    "        image = torch.zeros(1, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "    \n",
    "    # D√©placer sur le device\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    image = image.to(device)\n",
    "    \n",
    "    # Pr√©diction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask, image)\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    # R√©sultats\n",
    "    prediction = predicted.item()\n",
    "    fake_prob = probabilities[0][0].item()\n",
    "    real_prob = probabilities[0][1].item()\n",
    "    \n",
    "    return {\n",
    "        'prediction': prediction,\n",
    "        'label': 'REAL' if prediction == 1 else 'FAKE',\n",
    "        'fake_probability': fake_prob,\n",
    "        'real_probability': real_prob,\n",
    "        'confidence': max(fake_prob, real_prob)\n",
    "    }\n",
    "\n",
    "def test_article_prediction():\n",
    "    \"\"\"\n",
    "    Teste la pr√©diction sur quelques articles du jeu de test\n",
    "    \"\"\"\n",
    "    print(\"üß™ TEST DE PR√âDICTION EN TEMPS R√âEL\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Charger le meilleur mod√®le\n",
    "    model.load_state_dict(torch.load('best_multimodal_model.pth'))\n",
    "    \n",
    "    # Prendre quelques exemples du jeu de test\n",
    "    test_samples = test_df.sample(n=3, random_state=42)\n",
    "    \n",
    "    for idx, (_, row) in enumerate(test_samples.iterrows()):\n",
    "        print(f\"\\nüì∞ ARTICLE {idx+1}:\")\n",
    "        print(f\"   üìù Titre: {row['title'][:100]}...\")\n",
    "        print(f\"   üìÑ Texte: {row['text'][:150]}...\")\n",
    "        print(f\"   üñºÔ∏è Image: {row['image_path']}\")\n",
    "        print(f\"   üè∑Ô∏è Vraie √©tiquette: {'REAL' if row['label'] == 1 else 'FAKE'}\")\n",
    "        \n",
    "        # Pr√©diction\n",
    "        result = predict_fake_news(\n",
    "            model, tokenizer, \n",
    "            row['title'], row['text'], row['image_path']\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nü§ñ PR√âDICTION:\")\n",
    "        print(f\"   üéØ Pr√©diction: {result['label']}\")\n",
    "        print(f\"   üìä Confiance: {result['confidence']:.3f}\")\n",
    "        print(f\"   üî¥ Prob. Fake: {result['fake_probability']:.3f}\")\n",
    "        print(f\"   üü¢ Prob. Real: {result['real_probability']:.3f}\")\n",
    "        \n",
    "        # V√©rification\n",
    "        true_label = 'REAL' if row['label'] == 1 else 'FAKE'\n",
    "        is_correct = result['label'] == true_label\n",
    "        print(f\"   ‚úÖ Correct: {'OUI' if is_correct else 'NON'}\")\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Test de pr√©diction\n",
    "test_article_prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Fonction interactive pour tester vos propres articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_custom_article(title, text, image_path):\n",
    "    \"\"\"\n",
    "    Interface simple pour tester un article personnalis√©\n",
    "    \"\"\"\n",
    "    print(\"üîç ANALYSE D'ARTICLE PERSONNALIS√â\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(f\"üìù Titre: {title}\")\n",
    "    print(f\"üìÑ Texte: {text[:200]}{'...' if len(text) > 200 else ''}\")\n",
    "    print(f\"üñºÔ∏è Image: {image_path}\")\n",
    "    \n",
    "    # Charger le mod√®le\n",
    "    model.load_state_dict(torch.load('best_multimodal_model.pth'))\n",
    "    \n",
    "    # Pr√©diction\n",
    "    result = predict_fake_news(model, tokenizer, title, text, image_path)\n",
    "    \n",
    "    print(f\"\\nü§ñ R√âSULTAT DE L'ANALYSE:\")\n",
    "    print(f\"   üéØ Classification: {result['label']}\")\n",
    "    print(f\"   üìä Niveau de confiance: {result['confidence']:.1%}\")\n",
    "    \n",
    "    if result['label'] == 'FAKE':\n",
    "        print(f\"   üî¥ Probabilit√© d'√™tre FAKE: {result['fake_probability']:.1%}\")\n",
    "        print(f\"   ‚ö†Ô∏è Cet article semble √™tre une FAKE NEWS\")\n",
    "    else:\n",
    "        print(f\"   üü¢ Probabilit√© d'√™tre REAL: {result['real_probability']:.1%}\")\n",
    "        print(f\"   ‚úÖ Cet article semble √™tre une VRAIE NEWS\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Exemple d'utilisation (d√©commentez et modifiez selon vos besoins)\n",
    "# result = predict_custom_article(\n",
    "#     title=\"Votre titre d'article ici\",\n",
    "#     text=\"Le contenu complet de votre article ici...\",\n",
    "#     image_path=\"chemin/vers/votre/image.jpg\"\n",
    "# )\n",
    "\n",
    "print(\"\\nüéâ NOTEBOOK TERMIN√â!\")\n",
    "print(\"\\nüìã R√âSUM√â DU PROJET:\")\n",
    "print(f\"   ‚úÖ Donn√©es charg√©es et pr√©trait√©es\")\n",
    "print(f\"   ‚úÖ Images t√©l√©charg√©es et trait√©es\")\n",
    "print(f\"   ‚úÖ Mod√®le multimodal entra√Æn√© (BERT + ResNet)\")\n",
    "print(f\"   ‚úÖ √âvaluation compl√®te effectu√©e\")\n",
    "print(f\"   ‚úÖ Interface de test disponible\")\n",
    "print(f\"\\nüöÄ Vous pouvez maintenant utiliser predict_custom_article() pour tester vos propres articles!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
